data: 
  train_path: /data/processed_data/train_manifest_HF.json
  val_path: /data/processed_data/dev_manifest_HF.json
  # test_path: /data/processed_data/root_test_manifest_hf.json
  test_path: /data/evaluation/whisper-large-v2-2/t5_eval_input_from_asr.json

train:
  output_dir: "/models"

  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2

  learning_rate: 0.0003 # higher learning rate as T5 is smaller
  lr_scheduler_type: "linear"
  warmup_steps: 1000
  num_train_epochs: 5

  fp16: true
  gradient_checkpointing: false
  predict_with_generate: true
  generation_max_length: 16

  eval_strategy: "steps"
  eval_steps: 5000
  save_strategy: "steps"
  save_steps: 5000
  logging_strategy: "steps"
  logging_steps: 5000
  report_to: ["tensorboard"]

  load_best_model_at_end: true
  metric_for_best_model: "semantic_similarity" # used to define which metric Trainer uses to decide "best model"
  greater_is_better: true
